{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Challenge 3: Arquitetura ML e Aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "*Problema a ser resolvido*\n",
    "\n",
    "Identificar a correlação entre dados de usuário (idade, gênero, ocupação), preferência por filmes e sua avaliação.\n",
    "\n",
    "A partir desse modelo, será criada uma API que poderá predizer determinado resultado baseado nos dados pertencentes de uma pessoa (idade, gênero, ocupação) e sugerir filmes que se encaixam com as preferencias do mesmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install xgboost\n",
    "%pip install requests\n",
    "%pip install boto3\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Perceptron\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conjunto de Dados*\n",
    "\n",
    "Usaremos o grupo de dados do Movielens para esse caso de estudo (https://files.grouplens.org/datasets/movielens/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "file_path = \"ml-1m.zip\"\n",
    "response = requests.get(url)\n",
    "with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('raw/ml-1m/users.dat',sep='::',header=None,names=['userid','gender','age','occupation','zipcode'],encoding='iso-8859-2')\n",
    "movie_data = pd.read_csv('raw/ml-1m/movies.dat',sep='::',header=None,names=['movieid','title','genres'],encoding='iso-8859-2')\n",
    "ratings_data = pd.read_csv('raw/ml-1m/ratings.dat',sep='::',header=None,usecols=[0,1,2],names=['userid','movieid','rating'],encoding='iso-8859-2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura de cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analise dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar os dados, podemos perceber que possuímos um ratio desbalanceado entre usuários homens (M) e usuárias mulheres (F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['gender'].value_counts().plot(kind='pie', autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de prosseguirmos uma outra informação que é interessante de analisarmos é a distribuição etária de nossos usuários. Vale a pena resaltar que a coluda idade (age) já está \"normalizada\", obedecendo a seguinte proporção:\n",
    "*  1:  Mais novos que 18 anos\n",
    "* 18:  18-24 anos\n",
    "* 25:  25-34 anos\n",
    "* 35:  35-44 anos\n",
    "* 45:  45-49 anos\n",
    "* 50:  50-55 anos\n",
    "* 56:  56+ anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.plot.hist(column=[\"age\"], by=\"gender\", figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisarmos os dados, mais precisamente a coluna *genres* do dataset de Filmes, é possível perceber que temos um problema de 'Multi-Label' nos nossos dados, onde um filme pode pertencer a 1 ou N gêneros de filmes. Para contornarmos essa situação, iremos converter essa coluna para uma lista de string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data['genres'] = movie_data[\"genres\"].str.split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, utilizaremos o **MultiLabelBinarizer** para transformar essa coluna em uma matriz binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "encoded_data = mlb.fit_transform(movie_data['genres'])\n",
    "dados_encodados = pd.DataFrame(encoded_data, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_encodados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente concatenaremos esse novo dataframe ao \"original\", podendo assim excluir a coluna '*genres*' de nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.concat([movie_data,dados_encodados], axis=1)\n",
    "movie_data = movie_data.drop(['genres'], axis=1)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partindo de um princípio parecido, a coluna Gender do dataset de usuário que está dividido em M e F, pode ser alterada um LabelEncoder para facilitar nosso treinamento de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "user_data['gender'] = label_encoder.fit_transform(user_data['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['gender'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge e limpeza dos dados\n",
    "\n",
    "Criaremos um dataset/sample contendo tanto dados de usuarios quanto as suas respectivas avaliacoes. para treinarmos e testarmos os nossos modelos posteriomente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_completos = pd.merge(user_data, ratings_data, on='userid')\n",
    "dados_completos = pd.merge(dados_completos, movie_data, on='movieid')\n",
    "dados_completos = dados_completos.sample(n=5000, random_state=23)\n",
    "dados_completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_modelo = dados_completos.drop(['movieid', 'userid', 'title', 'zipcode'], axis=1)\n",
    "dados_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_modelo.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separa dados de treino de dados de teste\n",
    "\n",
    "Agora, podemos começar a nossa jornada de validação e testes de alguma modelos, como temos a coluna rating, que funciona como a nota do especialista, podemos começar com um aprendizado supervisionado. \n",
    "\n",
    "Começaremos com modelos de regressão para tal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente separamos os nossos dados X (features) e Y (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dados_modelo.drop(columns=['rating'], axis=1)\n",
    "y = dados_modelo['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora separamos a parcela de treino (80%) e teste (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já deixaremos aqui, uma função para a avaliação do Modelo e uma função para busca dos melhores hiper parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avalia_modelo(y_test,y_pred, tipo_modelo):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    print(f'{tipo_modelo} - MSE: {mse} RMSE: {rmse} MAPE: {mape}')\n",
    "\n",
    "def procura_melhores_parametros(model, parametros, treino_x, treino_y):\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=parametros,\n",
    "                           cv=5,\n",
    "                           scoring='neg_mean_absolute_percentage_error')\n",
    "\n",
    "    # busca a melhor combinação de parametros, baseado nos dados de teste\n",
    "    grid_search.fit(treino_x, treino_y)\n",
    "\n",
    "    # retorna o melhor modelo\n",
    "    melhor_modelo = grid_search.best_estimator_\n",
    "    return melhor_modelo  \n",
    "\n",
    "def save_dataframe_as_dat_locally(dataframe, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='iso-8859-2') as f:\n",
    "         for index, row in dataframe.iterrows():\n",
    "            f.write('::'.join(map(str, row.values)) + '\\n')\n",
    "    print(f\"Arquivo {file_path} salvo com sucesso.\")\n",
    "\n",
    "def create_boto_client(bucket_name):\n",
    "    boto_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=os.getenv('s3_access_key_id'),\n",
    "        aws_secret_access_key=os.getenv('s3_secret_access_key'),\n",
    "        aws_session_token=os.getenv('s3_session_token')\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        boto_client.list_objects_v2(Bucket=bucket_name)\n",
    "        return boto_client\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***REGRESSÂO LINEAR***\n",
    "\n",
    "Começaremos os nossos testes, utilizando o LinearRegression do scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = procura_melhores_parametros(LinearRegression(), {}, x_train, y_train)\n",
    "lr_pred =  model_lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***KNN Regressor***\n",
    "\n",
    "Agora utilizaremos o KNN Regressor, contudo para esse modelo utilizaremos a função previamente criada para chegar no melhor número de \"vizinhos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parametros = {\n",
    "    'n_neighbors': [2, 5, 8, 10],\n",
    "    'metric':['euclidean']\n",
    "}\n",
    "\n",
    "model_knn = procura_melhores_parametros(KNeighborsRegressor(), parametros, x_train, y_train)\n",
    "knnregressor_pred = model_knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Support Vector Machine: Linear***\n",
    "\n",
    "O SVM é uma ferramenta robusta, poderosa e versátil. Indicada para problemas mais complexos e alcançar alta precisão, vamos ver como se sai em nosso caso de estudo. Mais uma vez obtendo o melhor modelo a partir do gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'C': [5, 10]\n",
    "}\n",
    "\n",
    "model_svm = procura_melhores_parametros(SVR(kernel='linear'), parametros, x_train, y_train)\n",
    "svr_pred =  model_svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Árvore de Decisão***\n",
    "\n",
    "A árvore de decisão contrói uma estrutura hierárquica de regras que dividem em subconjuntos baseados nas características, facilitando a identificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {}\n",
    "\n",
    "model_tree = procura_melhores_parametros(DecisionTreeClassifier(), parametros, x_train, y_train)\n",
    "decision_tree_pred =  model_tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Redes Neurais Artificiais (RNA)***\n",
    "\n",
    "As redes neurais artificiais são modelos computacionais que, atraves dos Perceptrons, imitam o funcionamento do cérebro humano para resolver problemas complexos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {}\n",
    "\n",
    "model_perceptron = procura_melhores_parametros(Perceptron(), parametros, x_train, y_train)\n",
    "perceptron_pred =  model_perceptron.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Multilayer Perceptron (MLP)***\n",
    "\n",
    "O multilayer percepton é um modelo de rede neural artificial com múltiplas camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {}\n",
    "\n",
    "model_mlp = procura_melhores_parametros(MLPClassifier(), parametros, x_train, y_train)\n",
    "mlp_pred =  model_mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***XGBoost***\n",
    "\n",
    "Finalizaremos nossos testes com diferente tipos de estimadores, fazendo uso do XGBoost. É um dos mais utilizados pelo mercado e academia, funciona atráves da construção sequencial de N árvores de decisão mais fracas, onde cada árvore aprende com o erro da anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    \"n_estimators\": [20, 50, 100],\n",
    "    \"max_depth\": [3, 6, 8],\n",
    "    \"learning_rate\": [0.1]\n",
    "}\n",
    "\n",
    "model_xgb = procura_melhores_parametros(XGBRegressor(), parametros, x_train, y_train)\n",
    "\n",
    "xgboost_pred = model_xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, uma vez que temos os estimadores treinados, mas definir através do MAPE. Qual modelo performa melhor em nosso cenário de estudo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avalia_modelo(y_test, lr_pred, \"LinearRegression\")\n",
    "avalia_modelo(y_test, knnregressor_pred, \"KNN Regressor\")\n",
    "avalia_modelo(y_test, svr_pred, \"SVM Linear\")\n",
    "avalia_modelo(y_test, xgboost_pred, \"XGBoost\")\n",
    "avalia_modelo(y_test, decision_tree_pred, \"DecisionTree\")\n",
    "avalia_modelo(y_test, perceptron_pred, \"Perceptron\")\n",
    "avalia_modelo(y_test, mlp_pred, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos finalmente exportar nosso modelo e avançar para a próxima etapa que é construir uma API que consiga predizer para um usuário de acordo com diferentes dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset e modelo para o S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'techchallengegp53'\n",
    "\n",
    "boto_client = create_boto_client(bucket_name)\n",
    "aws_connected = boto_client is not None\n",
    "\n",
    "print(f'AWS Connected {aws_connected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando sample no S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aws_connected:\n",
    "\n",
    "    user_data_buffer = StringIO()\n",
    "\n",
    "    for index, row in user_data.iterrows():\n",
    "        user_data_buffer.write('::'.join(map(str, row.values)) + '\\n')\n",
    "    boto_client.put_object(Bucket=bucket_name, Key=\"refined/user_data.dat\", Body=user_data_buffer.getvalue())\n",
    "\n",
    "    movie_data_buffer = StringIO()\n",
    "\n",
    "    for index, row in movie_data.iterrows():\n",
    "        movie_data_buffer.write('::'.join(map(str, row.values)) + '\\n')\n",
    "    boto_client.put_object(Bucket=bucket_name, Key=\"refined/movie_data.dat\", Body=movie_data_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva modelos na S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aws_connected:\n",
    "    modelos_salvar = {\n",
    "        'LinearRegression': model_lr,\n",
    "        'KNN Regressor': model_knn,\n",
    "        'SVM Linear': model_svm,\n",
    "        'XGBoost': model_xgb,\n",
    "        'DecisionTree': model_tree,\n",
    "        'Perceptron': model_perceptron,\n",
    "        'MLP': None\n",
    "    }\n",
    "\n",
    "    for nome_modelo, instancia_modelo in modelos_salvar.items():\n",
    "        modelo_buffer = pickle.dumps(instancia_modelo)\n",
    "        boto_client.put_object(Bucket=bucket_name, Key=f\"modelo/{nome_modelo}.sav\", Body=modelo_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar se existe conexão com o Bucket. \n",
    "Se não existir, savamos local no `refined`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not aws_connected:\n",
    "    bucket_folder = 'refined'\n",
    "\n",
    "    user_data_file_path = os.path.join(bucket_folder, 'user_data.dat')\n",
    "    movie_data_file_path = os.path.join(bucket_folder, 'movie_data.dat')\n",
    "\n",
    "    save_dataframe_as_dat_locally(user_data, user_data_file_path)\n",
    "    save_dataframe_as_dat_locally(movie_data, movie_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "pastas_remocao = ['raw', 'refined']\n",
    "\n",
    "for pasta in pastas_remocao:\n",
    "    if (os.path.exists(pasta)):\n",
    "        shutil.rmtree(pasta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
