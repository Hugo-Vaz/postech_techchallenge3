{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Challenge 3: Arquitetura ML e Aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducão\n",
    "\n",
    "*Problema a ser resolvido*\n",
    "\n",
    "Identificar a correlação entre dados de usuário (idade, gênero, região, ocupação), preferência por filmes e sua avaliação.\n",
    "A partir desse modelo, será criada uma API que poderá receber dois inputs diferentes:\n",
    "\n",
    "1. Dados pertencentes a uma pessoa (idade, gênero, ...) e partir deles e do modelo treinado ser capaz de sugerir filmes em que poderiam potencialmente ter interesse.\n",
    "\n",
    "2. Dados relacionados a um filmes (tempo de duração, gêneros, ano de lançamento) e partir deles e do modelo treinado ser capaz de indicar o grupo de maior interesse, para que um time de marketing, por exemplo saiba pra onde direcionar as campanhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conjunto de Dados*\n",
    "\n",
    "Usaremos o grupo de dados do Movielens para esse caso de estudo (https://files.grouplens.org/datasets/movielens/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install xgboost\n",
    "%pip install requests\n",
    "%pip install boto3\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Perceptron\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "file_path = \"ml-1m.zip\"\n",
    "response = requests.get(url)\n",
    "with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('dataset/ml-1m/users.dat',sep='::',header=None,names=['userid','gender','age','occupation','zipcode'],encoding='iso-8859-2')\n",
    "movie_data = pd.read_csv('dataset/ml-1m/movies.dat',sep='::',header=None,names=['movieid','title','genres'],encoding='iso-8859-2')\n",
    "ratings_data = pd.read_csv('dataset/ml-1m/ratings.dat',sep='::',header=None,usecols=[0,1,2],names=['userid','movieid','rating'],encoding='iso-8859-2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura de cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisa os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar os dados, podemos perceber que possuímos um ratio desbalanceado entre usuários homens (M) e usuárias mulheres (F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['gender'].value_counts().plot(kind='pie', autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de prosseguirmos uma outra informação que é interessante de analisarmos é a distribuição etária de nossos usuários. Vale a pena resaltar que a coluda idade (age) já está \"normalizada\", obedecendo a seguinte proporção:\n",
    "*  1:  Mais novos que 18 anos\n",
    "* 18:  18-24 anos\n",
    "* 25:  25-34 anos\n",
    "* 35:  35-44 anos\n",
    "* 45:  45-49 anos\n",
    "* 50:  50-55 anos\n",
    "* 56:  56+ anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.plot.hist(column=[\"age\"], by=\"gender\", figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisarmos os dados, mais precisamente a coluna *genres* do dataset de Filmes, é possível perceber que temos um problema de 'Multi-Label' nos nossos dados, onde um filme pode pertencer a 1 ou N gêneros de filmes. Para contornarmos essa situação, iremos converter essa coluna para uma lista de string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data[\"release_year\"] = [int(re.search(r'\\((\\d{4})\\)', x).group(1)) for x in movie_data[\"title\"]]\n",
    "movie_data['genres'] = movie_data[\"genres\"].str.split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, utilizaremos o **MultiLabelBinarizer** para transformar essa coluna em uma matriz binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "encoded_data = mlb.fit_transform(movie_data['genres'])\n",
    "dados_encodados = pd.DataFrame(encoded_data, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_encodados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente concatenaremos esse novo dataframe ao \"original\", podendo assim excluir a coluna '*genres*' de nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.concat([movie_data,dados_encodados], axis=1)\n",
    "movie_data = movie_data.drop(['genres'], axis=1)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partindo de um princípio parecido, a coluna Gender do dataset de usuário que está dividido em M e F, pode ser alterada um LabelEncoder para facilitar nosso treinamento de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "user_data['gender'] = label_encoder.fit_transform(user_data['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['gender'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge e limpeza dos dados\n",
    "\n",
    "Antes de começarmos a testar quais modelos podem nos ajudar a solucionar o nosso problema, ou até mesmo antes de padronizarmos/tratarmos os nossos dados, iremos criar um quarto Dataset, que irá conter o todo do cenário com o qual pretendemos trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_completos = pd.merge(user_data, ratings_data, on='userid')\n",
    "dados_completos = pd.merge(dados_completos, movie_data, on='movieid')\n",
    "dados_completos = dados_completos.sample(n=5000, random_state=23)\n",
    "dados_completos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_modelo = dados_completos.drop(['movieid','userid','title','zipcode'], axis=1)\n",
    "dados_modelo.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_modelo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_modelo.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separa dados de treino de dados de teste\n",
    "\n",
    "Agora, podemos começar a nossa jornada de validação e testes de alguma modelos, como temos a coluna rating, que funciona como a nota do especialista, podemos começar com um aprendizado supervisionado. Começaremos com modelos de regressão para tal. Contudo, antes de comerçamos com os treinamentos, iremos separar nossos dados em dados de treino e dados de teste, utilizando o train_test_split do scikitlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente separamos os nossos dados X (features) e Y (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dados_modelo.drop(columns=['rating'], axis=1)\n",
    "y = dados_modelo['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora separamos a parcela de treino (80%) e teste (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já deixaremos aqui, uma função para a avaliação do Modelo e uma função para busca dos melhores hiper parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avalia_modelo(y_test,y_pred, tipo_modelo):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    print(f'{tipo_modelo} - MSE: {mse} RMSE: {rmse} MAPE: {mape}')\n",
    "\n",
    "def procura_melhores_parametros(model, parametros, treino_x, treino_y):\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=parametros,\n",
    "                           cv=5,\n",
    "                           scoring='neg_mean_absolute_percentage_error')\n",
    "\n",
    "    # busca a melhor combinação de parametros, baseado nos dados de teste\n",
    "    grid_search.fit(treino_x, treino_y)\n",
    "\n",
    "    # retorna o melhor modelo\n",
    "    melhor_modelo = grid_search.best_estimator_\n",
    "    return melhor_modelo  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***REGRESSÂO LINEAR***\n",
    "\n",
    "Começaremos os nossos testes, utilizando o LinearRegression do scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = procura_melhores_parametros(LinearRegression(), {}, x_train, y_train)\n",
    "lr_pred =  model.predict(x_test)\n",
    "\n",
    "avalia_modelo(y_test, lr_pred, 'Linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***KNN Regressor***\n",
    "\n",
    "Agora utilizaremos o KNN Regressor, contudo para esse modelo utilizaremos a função previamente criada para chegar no melhor número de \"vizinhos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parametros = {\n",
    "    'n_neighbors': [2, 5, 8, 10],\n",
    "    'metric':['euclidean']\n",
    "}\n",
    "\n",
    "model = procura_melhores_parametros(KNeighborsRegressor(), parametros, x_train, y_train)\n",
    "knnregressor_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Support Vector Machine: Linear***\n",
    "\n",
    "O SVM é uma ferramenta robusta, poderosa e versátil. Indicada para problemas mais complexos e alcançar alta precisão, vamos ver como se sai em nosso caso de estudo. Mais uma vez obtendo o melhor modelo a partir do gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'C': [5, 25, 60, 100]\n",
    "}\n",
    "\n",
    "model = procura_melhores_parametros(SVR(kernel='linear'), parametros, x_train, y_train)\n",
    "svr_pred =  model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Árvore de Decisão***\n",
    "\n",
    "A árvore de decisão contrói uma estrutura hierárquica de regras que dividem em subconjuntos baseados nas características, facilitando a identificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {}\n",
    "\n",
    "model = procura_melhores_parametros(DecisionTreeClassifier(), parametros, x_train, y_train)\n",
    "decision_tree_pred =  model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Redes Neurais Artificiais (RNA)***\n",
    "\n",
    "As redes neurais artificiais são modelos computacionais que imitam o funcionamento do cérebro humano para resolver problemas complexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {}\n",
    "\n",
    "model = procura_melhores_parametros(Perceptron(), parametros, x_train, y_train)\n",
    "perceptron_pred =  model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Multilayer Perceptron (MLP)***\n",
    "\n",
    "O multilayer percepton é um modelo de rene neural artificial com múltiplas camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {}\n",
    "\n",
    "model = procura_melhores_parametros(MLPClassifier(), parametros, x_train, y_train)\n",
    "mlp_pred =  model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***XGBoost***\n",
    "\n",
    "Finalizaremos nossos testes com diferente tipos de estimadores, fazendo uso do XGBoost. É um dos mais utilizados pelo mercado e academia, funciona atráves da construção sequencial de N árvores de decisão mais fracas, onde cada árvore aprende com o erro da anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    \"n_estimators\": [20, 50, 100],\n",
    "    \"max_depth\": [3, 6, 8],\n",
    "    \"learning_rate\": [0.1]\n",
    "}\n",
    "\n",
    "model = procura_melhores_parametros(XGBRegressor(), parametros, x_train, y_train)\n",
    "\n",
    "xgboost_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, uma vez que temos os estimadores treinados, mas definir através do MAPE. Qual modelo performa melhor em nosso cenário de estudo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avalia_modelo(y_test,lr_pred,\"LinearRegression\")\n",
    "avalia_modelo(y_test,knnregressor_pred,\"KNN Regressor\")\n",
    "avalia_modelo(y_test,svr_pred,\"SVM Linear\")\n",
    "avalia_modelo(y_test,xgboost_pred,\"XGBoost\")\n",
    "avalia_modelo(y_test,decision_tree_pred,\"DecisionTree\")\n",
    "avalia_modelo(y_test,perceptron_pred,\"Perceptron\")\n",
    "avalia_modelo(y_test,mlp_pred,\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos que o melhor estimador pro nosso cenário de estudo, foi o {INSERIR AQUI MENOR O DE MENOR MAPE} cujo o MAPE foi de {INSERIR VALOR DO MAPE}. Lembrando que poderiamos tentar otimizar ainda mais, utilizando uma gama maior de hyper parâmetros, contudo poderiamos cair em cenário onde ficariamos extremamente especializados com nossos dados de teste (Overfitting).\n",
    "\n",
    "Podemos finalmente exportar nosso modelo e avançar para a próxima etapa que é construir uma API que consiga predizer para um usuário de acordo com diferentes dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria instancia do Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "bucket_name = 'techchallengegp53'\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    aws_session_token=os.getenv('AWS_SESSION_TOKEN')\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    bucket_connected = True\n",
    "except Exception as e:\n",
    "    bucket_connected = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar se existe conexão com o Bucket. Se não existir, savamos local no FakeBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_as_dat_locally(dataframe, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='iso-8859-2') as f:\n",
    "         for index, row in dataframe.iterrows():\n",
    "            f.write('::'.join(map(str, row.values)) + '\\n')\n",
    "    print(f\"Arquivo {file_path} salvo com sucesso.\")\n",
    "\n",
    "bucket_folder = 'fakeBucket'\n",
    "user_data_file_path = os.path.join(bucket_folder, 'user_data.dat')\n",
    "movie_data_file_path = os.path.join(bucket_folder, 'movie_data.dat')\n",
    "\n",
    "if not bucket_connected:\n",
    "    save_dataframe_as_dat_locally(user_data, user_data_file_path)\n",
    "    save_dataframe_as_dat_locally(movie_data, movie_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando movie_data em um bucket S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bucket_connected:\n",
    "\n",
    "    movie_data_buffer = StringIO()\n",
    "\n",
    "    for index, row in movie_data.iterrows():\n",
    "        movie_data_buffer.write('::'.join(map(str, row.values)) + '\\n')\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=\"movie_data.dat\", Body=movie_data_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "pastas_remocao = ['dataset', 'fakeBucket']\n",
    "\n",
    "for pasta in pastas_remocao:\n",
    "    if (os.path.exists(pasta)):\n",
    "        shutil.rmtree(pasta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
